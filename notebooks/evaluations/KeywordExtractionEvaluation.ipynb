{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fcKbVAjcxnR",
        "outputId": "b4172e77-043a-460b-d160-c5aaf5d3dba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 60 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 35.2 MB/s \n",
            "\u001b[?25h  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 59.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 30.9 MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 49.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 947 kB 42.4 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "\u001b[?25h2022-08-17 01:05:05.567478: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 18.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet wikipedia-api\n",
        "!pip install --quiet rake-nltk\n",
        "!pip install --quiet yake\n",
        "!pip install --quiet keybert\n",
        "!pip install --quiet pytextrank\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5N3pKszZzbi",
        "outputId": "8a85d84d-e4af-4d1f-d9ab-7f21b89ae46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f6381cc7b10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "from operator import itemgetter\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "from bisect import bisect_left\n",
        "from collections import Counter\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from rake_nltk import Rake\n",
        "from keybert import KeyBERT\n",
        "import yake\n",
        "import spacy\n",
        "import pytextrank\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import random\n",
        "import os\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si1CxI8SdagI"
      },
      "outputs": [],
      "source": [
        "wiki = wikipediaapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW1YXY1PfZFN"
      },
      "outputs": [],
      "source": [
        "def _most_frequent_words(text : list):\n",
        "    freq = FreqDist(text)\n",
        "    return sorted(freq.items(), key=itemgetter(1), reverse=True)[:20]\n",
        "\n",
        "def _get_n_grams(text : string, n : int):\n",
        "    result = []\n",
        "    n_grams = ngrams(text.split(), n)\n",
        "    for grams in n_grams :\n",
        "        result.append(grams)\n",
        "    return result\n",
        "\n",
        "def _is_duplicate(keyword : string, keywords : list):\n",
        "  \n",
        "    if [wiki.page(x).text[:50] for x in keywords].count(wiki.page(keyword).text[:50]) >= 1:\n",
        "        return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "\n",
        "def _remove_duplicates(keywords : list):\n",
        "    for keyword in keywords:\n",
        "        if _is_duplicate(keyword, keywords):\n",
        "            keywords.remove(keyword)\n",
        "    return keywords\n",
        "\n",
        "def _topic_relevance_score(word, keywords : list):\n",
        "        score = 0\n",
        "        for y in keywords:\n",
        "            if y != word:\n",
        "                score += [x.lower() for x in word_tokenize(wiki.page(word).text)].count(y)\n",
        "        return score\n",
        "\n",
        "def _get_stopwords():\n",
        "    file = open(\"stopwords.txt\", \"rb\")\n",
        "    stopwords = []\n",
        "    for word in file:\n",
        "        stopwords.append(SnowballStemmer(\"english\").stem(re.sub('\\n', '', word.decode(\"utf-8\"))))\n",
        "    return stopwords\n",
        "\n",
        "def _remove_stopwords(document : string):\n",
        "    stopwords = _get_stopwords()\n",
        "    words = [word.lower() for word in word_tokenize(document) if SnowballStemmer(\"english\").stem(word.lower()) not in stopwords and word.isalpha() is True]\n",
        "    return \" \".join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viDLGEI1S6o0"
      },
      "outputs": [],
      "source": [
        "def get_doc_keywords(text:string):\n",
        "\n",
        "    bigrams = Counter(_get_n_grams(_remove_stopwords(text), 2)).most_common(10)\n",
        "    trigrams = Counter(_get_n_grams(_remove_stopwords(text), 2)).most_common(10)\n",
        "    keywords = [] + [x[0][0] + \" \" + x[0][1] for x in bigrams] + [x[0][0] + \" \" + x[0][1] + x[0][1] for x in trigrams]\n",
        "    keywords.append(_most_frequent_words(word_tokenize(_remove_stopwords(text)))[0][0])\n",
        "    keywords = keywords + [x[0] for x in _most_frequent_words([x for y in trigrams for x in y[0]])[:3]]\n",
        "    keywords = keywords + [x[0] for x in _most_frequent_words([x for y in bigrams for x in y[0]])[:3]]\n",
        "   \n",
        "    keywords = [x for x in keywords if wiki.page(x).exists() and wiki.page(x).text[:100].find(\"refer to\") == -1]\n",
        "    \n",
        "    keywords = _remove_duplicates(keywords)\n",
        "\n",
        "    topic_relevance_scores = {}\n",
        "        \n",
        "    for kw in keywords:\n",
        "      topic_relevance_scores[kw] = _topic_relevance_score(kw, keywords)  \n",
        "    \n",
        "    keywords = [x for x in topic_relevance_scores if topic_relevance_scores[x] >= 10]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def rake_extraction(row):\n",
        "\n",
        "  text = row[\"text\"]\n",
        "  \n",
        "  n_keywords = row[\"number_of_keywords\"]\n",
        "\n",
        "  r = Rake(max_length= 3, min_length = 1, include_repeated_phrases=False)\n",
        "\n",
        "  # Extraction given the text.\n",
        "  r.extract_keywords_from_text(text)\n",
        "\n",
        "  # Get keyword phrases ranked highest to lowest with scores.\n",
        "  keywords = [x[1] for x in r.get_ranked_phrases_with_scores()][:n_keywords]\n",
        "\n",
        "  keywords = [x for x in keywords if wiki.page(x).exists() and wiki.page(x).text[:100].find(\"refer to\") == -1]\n",
        "\n",
        "  keywords = _remove_duplicates(keywords)\n",
        "  \n",
        "  return keywords\n",
        "  \n",
        "\n",
        "\n",
        "def keybert_extraction(row):\n",
        "  text = row[\"text\"]\n",
        "  n_keywords = row[\"number_of_keywords\"]\n",
        "  text = _remove_stopwords(text)\n",
        "  kw_model = KeyBERT()\n",
        "  keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3),use_maxsum=True, nr_candidates=n_keywords, top_n=n_keywords)\n",
        "  keywords = [x[0] for x in keywords]\n",
        "  keywords = [x for x in keywords if wiki.page(x).exists() and wiki.page(x).text[:100].find(\"refer to\") == -1]\n",
        "  keywords = _remove_duplicates(keywords)\n",
        "  return keywords\n",
        "\n",
        "def yake_extraction(row):\n",
        "  text = row[\"text\"]\n",
        "  n_keywords = row[\"number_of_keywords\"]\n",
        "  text = _remove_stopwords(text)\n",
        "  simple_kwextractor = yake.KeywordExtractor(n = 3, top = n_keywords)\n",
        "  keywords = [x[0] for x in simple_kwextractor.extract_keywords(text)]\n",
        "  keywords = [x for x in keywords if wiki.page(x).exists() and wiki.page(x).text[:100].find(\"refer to\") == -1]\n",
        "  keywords = _remove_duplicates(keywords)\n",
        "  return keywords\n",
        "\n",
        "def textrank_extraction(row):\n",
        "\n",
        "  text = row[\"text\"]\n",
        "  n_keywords = row[\"number_of_keywords\"]\n",
        "  \n",
        "  # load a spaCy model, depending on language, scale, etc.\n",
        "  text = _remove_stopwords(text)\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  # add PyTextRank to the spaCy pipeline\n",
        "  nlp.add_pipe(\"textrank\")\n",
        "  doc = nlp(text)\n",
        "\n",
        "  keywords = []\n",
        "\n",
        "  # examine the top-ranked phrases in the document\n",
        "  for phrase in doc._.phrases:\n",
        "      keywords.append(phrase.text)\n",
        "\n",
        "  keywords = keywords[:n_keywords]\n",
        "\n",
        "  keywords = [x for x in keywords if wiki.page(x).exists() and wiki.page(x).text[:100].find(\"refer to\") == -1]\n",
        "  keywords = _remove_duplicates(keywords)\n",
        "\n",
        "  return keywords\n",
        "\n",
        "def to_list(list_string):\n",
        "  return [x.strip().strip('\"').strip(\"'\").strip() for x in list_string.strip(\"[\").strip(\"]\").split(\",\")]\n",
        "\n",
        "def get_dataset():\n",
        "  \n",
        "  data = {}\n",
        "\n",
        "  all_keywords = []\n",
        "\n",
        "  all_text = []\n",
        "\n",
        "  all_filename = []\n",
        "\n",
        "  for f_name in os.listdir(\"Nguyen2007/docsutf8\"):\n",
        "    text = open(\"Nguyen2007/docsutf8/\" + f_name, \"r\").read()\n",
        "    f_key = f_name.split(\".\")[0] + \".key\"\n",
        "    keywords = open(\"Nguyen2007/keys/\" + f_key, \"r\").read().split(\"\\n\")\n",
        "    all_keywords.append(keywords)\n",
        "    all_text.append(text)\n",
        "    all_filename.append(f_name)\n",
        "\n",
        "  data[\"keywords\"] = all_keywords\n",
        "  data[\"filename\"] = all_filename\n",
        "  data[\"text\"] = all_text\n",
        "  data[\"number_of_keywords\"] = [len(x) for x in data[\"keywords\"]]\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "\n",
        "def precision(truth:list, predictions:list):\n",
        "  if len(predictions) == 0:\n",
        "    return 0\n",
        "  predictions = [normalize_text(x) for x in predictions]\n",
        "  truth = [normalize_text(x) for x in truth]\n",
        "  common_keywords = list(set(truth) & set(predictions))\n",
        "  return len(common_keywords) / len(predictions)\n",
        "\n",
        "def recall(truth:list, predictions:list):\n",
        "  if len(predictions) == 0:\n",
        "    return 0\n",
        "  predictions = [normalize_text(x) for x in predictions]\n",
        "  truth = [normalize_text(x) for x in truth]\n",
        "  common_keywords = list(set(truth) & set(predictions))\n",
        "  return len(common_keywords) / len(truth)\n",
        "\n",
        "def f1_score(truth:list, predictions:list):\n",
        "  prec = precision(truth, predictions)\n",
        "  rec = recall(truth, predictions)\n",
        "  if prec + rec == 0:\n",
        "    return 0\n",
        "  return 2 * (prec * rec) / (prec + rec)"
      ],
      "metadata": {
        "id": "vwKMDxOSAZiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Nguyen2007.zip"
      ],
      "metadata": {
        "id": "c9lP-OaopY2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oKCZ2YVXl3-"
      },
      "outputs": [],
      "source": [
        "df = get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"rake\"] = df.apply(lambda x: rake_extraction(x), axis = 1)"
      ],
      "metadata": {
        "id": "xJ4jqC-zpZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"yake\"] = df.apply((lambda x: yake_extraction(x)), axis = 1)"
      ],
      "metadata": {
        "id": "QKOlXRWpz5Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"keybert\"] = df.apply((lambda x: keybert_extraction(x)), axis = 1)"
      ],
      "metadata": {
        "id": "olTewFq3z5De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"textrank\"] = df.apply((lambda x: textrank_extraction(x)), axis = 1)"
      ],
      "metadata": {
        "id": "1n2haCFKz6xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"keywords.xlsx\")"
      ],
      "metadata": {
        "id": "RLZIOd7NQoBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truth = [[x for x in to_list(x) if x != ''] for x in list(df[\"keywords\"])]"
      ],
      "metadata": {
        "id": "534J_E4APdz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rake_predictions = [[x for x in to_list(x) if x != ''] for x in list(df[\"rake\"])]\n",
        "rake = list(zip(truth, rake_predictions))\n",
        "\n",
        "rake_precision = [precision(x[0], x[1]) for x in rake]\n",
        "rake_recall = [recall(x[0], x[1]) for x in rake]\n",
        "rake_f1 = [f1_score(x[0], x[1]) for x in rake]\n",
        "\n",
        "print(\"Precision: \", sum(rake_precision)/len(rake_precision))\n",
        "print(\"Recall: \", sum(rake_recall)/len(rake_recall))\n",
        "print(\"F1-score: \", sum(rake_f1)/len(rake_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qeb9uFTJLDW",
        "outputId": "2f2e42c6-0790-478c-b6c9-7a1da0a91758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.0021738698853350723\n",
            "Recall:  0.002378419031428276\n",
            "F1-score:  0.0022681438682367106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yake_predictions = [[x for x in to_list(x) if x != ''] for x in list(df[\"yake\"])]\n",
        "yake = list(zip(truth, yake_predictions))\n",
        "\n",
        "yake_precision = [precision(x[0], x[1]) for x in yake]\n",
        "yake_recall = [recall(x[0], x[1]) for x in yake]\n",
        "yake_f1 = [f1_score(x[0], x[1]) for x in yake]\n",
        "\n",
        "print(\"Precision: \", sum(yake_precision)/len(yake_precision))\n",
        "print(\"Recall: \", sum(yake_recall)/len(yake_recall))\n",
        "print(\"F1-score: \", sum(yake_f1)/len(yake_f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzeGspnsAZqz",
        "outputId": "5359739b-2dec-49c2-9072-8eafb1880583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.02804980900413081\n",
            "Recall:  0.029697346297601117\n",
            "F1-score:  0.028759857113784416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keybert_predictions = [[x for x in to_list(x) if x != ''] for x in list(df[\"keybert\"])]\n",
        "keybert = list(zip(truth, keybert_predictions))\n",
        "\n",
        "keybert_precision = [precision(x[0], x[1]) for x in keybert]\n",
        "keybert_recall = [recall(x[0], x[1]) for x in keybert]\n",
        "keybert_f1 = [f1_score(x[0], x[1]) for x in keybert]\n",
        "\n",
        "print(\"Precision: \", sum(keybert_precision)/len(keybert_precision))\n",
        "print(\"Recall: \", sum(keybert_recall)/len(keybert_recall))\n",
        "print(\"F1-score: \", sum(keybert_f1)/len(keybert_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSGHwhEIPM0z",
        "outputId": "0a8291e2-49d3-4e85-d92d-bc7f95531ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.009289651387289679\n",
            "Recall:  0.009865442333894024\n",
            "F1-score:  0.009551831421156837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textrank_predictions = [[x for x in to_list(x) if x != ''] for x in list(df[\"textrank\"])]\n",
        "textrank = list(zip(truth, textrank_predictions))\n",
        "\n",
        "textrank_precision = [precision(x[0], x[1]) for x in textrank]\n",
        "textrank_recall = [recall(x[0], x[1]) for x in textrank]\n",
        "textrank_f1 = [f1_score(x[0], x[1]) for x in textrank]\n",
        "\n",
        "print(\"Precision: \", sum(textrank_precision)/len(textrank_precision))\n",
        "print(\"Recall: \", sum(textrank_recall)/len(textrank_recall))\n",
        "print(\"F1-score: \", sum(textrank_f1)/len(textrank_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3U605ElQqar",
        "outputId": "d4f39223-ad01-4f28-e321-cf24a3e07246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.014169661294425392\n",
            "Recall:  0.01444934520414026\n",
            "F1-score:  0.014300345336592689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "louis_predictions = [[x for x in to_list(x) if x != ''] for x in list(df[\"louis_algo\"])]\n",
        "louis = list(zip(truth, louis_predictions))\n",
        "\n",
        "louis_precision = [precision(x[0], x[1]) for x in louis]\n",
        "louis_recall = [recall(x[0], x[1]) for x in louis]\n",
        "louis_f1 = [f1_score(x[0], x[1]) for x in louis]\n",
        "\n",
        "print(\"Precision: \", sum(louis_precision)/len(louis_precision))\n",
        "print(\"Recall: \", sum(louis_recall)/len(louis_recall))\n",
        "print(\"F1-score: \", sum(louis_f1)/len(louis_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZHw6ol0Q6Nj",
        "outputId": "3d0c8c9b-d9b5-49b3-a8c9-3630a771ded8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.07097288676236045\n",
            "Recall:  0.012190792711182386\n",
            "F1-score:  0.017737858318188995\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "KeywordExtractionEvaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}