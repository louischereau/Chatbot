{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQ48hI0G6wN",
        "outputId": "4f10325c-f2df-4f75-bd2b-8e599625d8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 29.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 61.6 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 28.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[K     |████████████████████████████████| 816 kB 71.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 141 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 70.3 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 61.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 62.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 365 kB 70.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 65.6 MB/s \n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet transformers==4.1.1\n",
        "!pip install --quiet torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2\n",
        "!pip install --quiet tokenizers==0.9.4\n",
        "!pip install --quiet sentencepiece==0.1.94\n",
        "!pip install --quiet evaluate\n",
        "!pip install --quiet rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAs8HNuhHRHr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random \n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "import requests  \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "'''\n",
        "optimizer - AdamW\n",
        "T5 Conditional Generator in which we'll give conditions\n",
        "T5 tokenizer because it is fast\n",
        "training the model without a learning rate\n",
        "'''\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from gc import collect\n",
        "\n",
        "import evaluate\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxGrlW7vTJT6"
      },
      "outputs": [],
      "source": [
        "def prepare_input_qa(row, dataset):\n",
        "  if dataset == \"squad\":\n",
        "    return \"question: %s context: %s </s>\" % (row[\"question\"], row[\"context\"])\n",
        "  if dataset == \"hotpotqa\":\n",
        "    return \"question: %s context1: %s context2: %s </s>\" % (row[\"question\"], row[\"context1\"], row[\"context2\"])\n",
        "\n",
        "def prepare_input_qg(row, dataset):\n",
        "  if dataset == \"squad\":\n",
        "    return \"answer: %s context: %s </s>\" % (row[\"answer\"], row[\"context\"])\n",
        "  if dataset == \"hotpotqa\":\n",
        "    return \"answer: %s context1: %s context2: %s </s>\" % (row[\"answer\"], row[\"context1\"], row[\"context2\"])\n",
        "\n",
        "\n",
        "def extract_questions_and_answers(factoid_path = Path):\n",
        "  with factoid_path.open() as json_file:\n",
        "    data = json.load(json_file)\n",
        "    questions = data['data'][0]['paragraphs']\n",
        "    data_rows = []\n",
        "    for question in questions:\n",
        "      context = question['context']\n",
        "      for question_and_answers in question['qas']:\n",
        "        question = question_and_answers['question']\n",
        "        answers = question_and_answers['answers']\n",
        "        for answer in answers:\n",
        "          answer_text = answer['text']\n",
        "          answer_start = answer['answer_start']\n",
        "          answer_end = answer['answer_start'] + len(answer_text)  #Gets the end index of each answer in the paragraph\n",
        "          \n",
        "          data_rows.append({\n",
        "                \"question\" : question,\n",
        "                \"context\"  : context,\n",
        "                \"answer\" : answer_text,\n",
        "                \"answer_start\" : answer_start,\n",
        "                \"answer_end\" : answer_end\n",
        "            })\n",
        "  return pd.DataFrame(data_rows)\n",
        "  \n",
        "\n",
        "def get_squad():\n",
        "\n",
        "  url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "\n",
        "  for file in [\"train-v2.0.json\", \"dev-v2.0.json\", \"train-v1.1.json\", \"dev-v1.1.json\"]:\n",
        "    res = requests.get(f'{url}{file}')\n",
        "    with open(f'squad/{file}', \"wb\") as f:\n",
        "      for chunk in res.iter_content(chunk_size=4):\n",
        "        f.write(chunk)\n",
        "\n",
        "  factoid_path_train2 = Path(\"squad/train-v2.0.json\")\n",
        "  factoid_path_dev2 = Path(\"squad/dev-v2.0.json\")\n",
        "  factoid_path_train1 = Path(\"squad/train-v1.1.json\")\n",
        "  factoid_path_dev1 = Path(\"squad/dev-v1.1.json\") \n",
        "\n",
        "  dev_df1 = extract_questions_and_answers(factoid_path_dev1) \n",
        "  train_df1 = extract_questions_and_answers(factoid_path_train1) \n",
        "  train_df2 = extract_questions_and_answers(factoid_path_train2)\n",
        "  dev_df2 = extract_questions_and_answers(factoid_path_dev2)\n",
        "\n",
        "  df = dev_df1.append(train_df1.append(train_df2.append(dev_df2)))\n",
        "\n",
        "  df.drop_duplicates(subset=['question', 'context'], inplace=True)\n",
        "\n",
        "  df[\"input_qg\"] = df.apply(lambda row: prepare_input_qg(row, \"squad\"), axis=1)\n",
        "  df[\"input_qa\"] = df.apply(lambda row: prepare_input_qa(row, \"squad\"), axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def filter_context(row):\n",
        "  supporting_facts = list(set([x[0] for x in row[\"supporting_facts\"]]))\n",
        "  contexts = []\n",
        "  for fact in supporting_facts:\n",
        "    for context in row[\"context\"]:\n",
        "      if fact in context[0] or context[0] in fact:\n",
        "        contexts.append(context[1])\n",
        "  row[\"context\"] = contexts\n",
        "  return row\n",
        "\n",
        "def seperate_context(row):\n",
        "  contexts = row[\"context\"]\n",
        "  row[\"context1\"] = \" \".join(contexts[0])\n",
        "  row[\"context2\"] = \" \".join(contexts[1])\n",
        "  return row\n",
        "\n",
        "def get_hotpotqa():\n",
        "\n",
        "  res = requests.get(\"http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json\")\n",
        "  \n",
        "  with open('hotpotqa/hotpot_train_v1.1.json', \"wb\") as f:\n",
        "    for chunk in res.iter_content(chunk_size=4):\n",
        "      f.write(chunk)\n",
        "\n",
        "  df = pd.read_json(\"hotpotqa/hotpot_train_v1.1.json\")\n",
        "\n",
        "  df = df.sample(1917, random_state = 0)\n",
        "  original_contexts = list(df[\"context\"])\n",
        "  df = df.apply(filter_context, axis = 1)\n",
        "  df = df.apply(seperate_context, axis = 1)\n",
        "  df[\"context\"] = original_contexts\n",
        "  df[\"input_qg\"] = df.apply(lambda row: prepare_input_qg(row, \"hotpotqa\"), axis=1)\n",
        "  df[\"input_qa\"] = df.apply(lambda row: prepare_input_qa(row, \"hotpotqa\"), axis=1)\n",
        "  return df\n",
        "  \n",
        "\n",
        "def create_folds(df : pd.DataFrame, dataset_name : string):\n",
        "\n",
        "  kf = KFold(n_splits=7, random_state=33, shuffle=True)\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for train_index, test_index in kf.split(df):\n",
        "    i += 1\n",
        "    train, test = df.iloc[train_index,:], df.iloc[test_index,:]\n",
        "    os.makedirs(dataset_name + \"_folds/\" + dataset_name + \"_fold\" + str(i))\n",
        "    train, val = train_test_split(train, test_size = test.shape[0]/train.shape[0])\n",
        "    train.to_csv(dataset_name + \"_folds/\" + dataset_name + \"_fold\" + str(i) + \"/train.csv\")\n",
        "    val.to_csv(dataset_name + \"_folds/\" + dataset_name + \"_fold\" + str(i) + \"/val.csv\")\n",
        "    test.drop(columns = [\"input_qa\"], inplace = True)\n",
        "    test.to_csv(dataset_name + \"_folds/\" + dataset_name + \"_fold\" + str(i) + \"/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL9NvqvuHhkx"
      },
      "outputs": [],
      "source": [
        "class SquadDataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data:pd.DataFrame,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      source_max_token_len: int = 396,\n",
        "      target_max_token_len: int = 32,\n",
        "\n",
        "      ):\n",
        "    \n",
        "    self.data =  data\n",
        "    self.tokenizer =  tokenizer\n",
        "    self.source_max_token_len =  source_max_token_len\n",
        "    self.target_max_token_len =  target_max_token_len\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "\n",
        "    source_encoding = self.tokenizer(\n",
        "      data_row[\"input_qg\"],\n",
        "      max_length=self.source_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=\"only_first\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    \n",
        "    target_encoding = self.tokenizer(\n",
        "      data_row['question'],\n",
        "      max_length=self.target_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    \n",
        "    labels = target_encoding['input_ids']\n",
        "    labels[labels==0] = -100\n",
        "\n",
        "    return dict(\n",
        "        question=data_row['question'],\n",
        "        input_text = data_row[\"input_qg\"],\n",
        "        input_ids=source_encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask=source_encoding['attention_mask'].flatten(),\n",
        "        labels=labels.flatten()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8YZC_wKHsiJ"
      },
      "outputs": [],
      "source": [
        "class SquadDataModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      train_df: pd.DataFrame,\n",
        "      test_df: pd.DataFrame,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      batch_size: int = 8,\n",
        "      source_max_token_len: int = 396,\n",
        "      target_max_token_len: int = 32,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.train_df = train_df\n",
        "    self.test_df = test_df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.source_max_token_len = source_max_token_len\n",
        "    self.target_max_token_len = target_max_token_len\n",
        "\n",
        "  def setup(self):\n",
        "    self.train_dataset = SquadDataset(\n",
        "        self.train_df,\n",
        "        self.tokenizer,\n",
        "        self.source_max_token_len,\n",
        "        self.target_max_token_len\n",
        "        )\n",
        "\n",
        "    self.test_dataset = SquadDataset(\n",
        "    self.test_df,\n",
        "    self.tokenizer,\n",
        "    self.source_max_token_len,\n",
        "    self.target_max_token_len\n",
        "    )\n",
        " \n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "        )\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        num_workers=4\n",
        "        )\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        num_workers=4\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoEoBgHxHvSq"
      },
      "outputs": [],
      "source": [
        "class SquadModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    output = self.model(\n",
        "        input_ids, \n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels)\n",
        "\n",
        "    return output.loss, output.logits\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = AdamW(self.parameters(), lr=0.0001)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHpyQtwHHyOi"
      },
      "outputs": [],
      "source": [
        "def generate_question(context, trained_model, tokenizer, dataset_name):\n",
        "\n",
        "  if dataset_name == \"squad\":\n",
        "    source_max_len = 386\n",
        "    target_max_len = 34\n",
        "  \n",
        "  if dataset_name == \"hotpotqa\":\n",
        "    source_max_len = 528\n",
        "    target_max_len = 100\n",
        "  \n",
        "  source_encoding=tokenizer(\n",
        "      context[\"input_qg\"],\n",
        "      max_length=source_max_len,\n",
        "      padding=\"max_length\",\n",
        "      truncation=\"only_first\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  generated_ids = trained_model.model.generate(\n",
        "      input_ids=source_encoding[\"input_ids\"],\n",
        "      attention_mask=source_encoding[\"attention_mask\"],\n",
        "      num_beams=1,  # greedy search\n",
        "      max_length=target_max_len,\n",
        "      repetition_penalty=2.5,\n",
        "      early_stopping=True,\n",
        "      use_cache=True)\n",
        "  \n",
        "  preds = [\n",
        "          tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "          for generated_id in generated_ids\n",
        "  ]\n",
        "\n",
        "  return \"\".join(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22YLYHlrVper"
      },
      "outputs": [],
      "source": [
        "def calculate_scores(test:pd.DataFrame):\n",
        "\n",
        "  rouge = evaluate.load('rouge')\n",
        "  bleu = evaluate.load(\"bleu\")\n",
        "  meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "  bleu_scores = []\n",
        "  rouge1_scores = []\n",
        "  rouge2_scores = []\n",
        "  rougeL_scores = []\n",
        "  meteor_scores = []\n",
        "\n",
        "  for i in range(len(test)):\n",
        "    predictions = [test.iloc[i, :][\"t5_questions\"]]\n",
        "    references = [test.iloc[i, :][\"question\"]]\n",
        "    bleu_scores.append(round(bleu.compute(predictions=predictions, references=references)[\"bleu\"],2)*100)\n",
        "    rouge1_scores.append(round(rouge.compute(predictions=predictions, references=references)[\"rouge1\"],2)*100)\n",
        "    rouge2_scores.append(round(rouge.compute(predictions=predictions, references=references)[\"rouge2\"],2)*100)\n",
        "    rougeL_scores.append(round(rouge.compute(predictions=predictions, references=references)[\"rougeL\"],2)*100)\n",
        "    meteor_scores.append(round(meteor.compute(predictions=predictions, references=references)[\"meteor\"], 2)*100)\n",
        "  \n",
        "  bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
        "  rouge1_score = round(sum(rouge1_scores) / len(rouge1_scores), 2)\n",
        "  rouge2_score = round(sum(rouge2_scores) / len(rouge2_scores), 2)\n",
        "  rougeL_score = round(sum(rougeL_scores) / len(rougeL_scores), 2)\n",
        "  meteor_score = round(sum(meteor_scores) / len(meteor_scores), 2)\n",
        "\n",
        "  return bleu_score, rouge1_score, rouge2_score, rougeL_score, meteor_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7o1r2CkH5_y"
      },
      "outputs": [],
      "source": [
        "def cross_val(dataset_name : string):\n",
        "\n",
        "  if dataset_name == \"squad\":\n",
        "    source_max_token_len=387\n",
        "    target_max_token_len=34\n",
        "  \n",
        "  if dataset_name == \"hotpotqa\":\n",
        "    source_max_token_len=528\n",
        "    target_max_token_len=100\n",
        "\n",
        "  for i in range(7, len(os.listdir(dataset_name + \"_folds\"))+1):\n",
        "    pl.seed_everything(0)\n",
        "    collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = SquadModel()\n",
        "    path = dataset_name + \"_folds/\" + dataset_name + \"_fold\" + str(i) + \"/\"\n",
        "    train = pd.read_csv(path + \"train.csv\")\n",
        "    val = pd.read_csv(path + \"val.csv\")\n",
        "    test = pd.read_csv(path + \"test.csv\")\n",
        "\n",
        "    \n",
        "    data_module = SquadDataModule(train, val, tokenizer, batch_size=BATCH_SIZE, source_max_token_len=source_max_token_len, target_max_token_len=target_max_token_len)\n",
        "    data_module.setup()\n",
        "\n",
        "    trainer.fit(model, data_module)\n",
        "\n",
        "    trained_model = SquadModel.load_from_checkpoint(\"checkpoints/bestcheckpoint.ckpt\")\n",
        "    trained_model.freeze()\n",
        "\n",
        "    questions = []\n",
        "\n",
        "    for i in range(len(test)):\n",
        "      questions.append(generate_question(test.iloc[i,:], trained_model, tokenizer, dataset_name))\n",
        "\n",
        "    test[\"t5_questions\"] = questions\n",
        "\n",
        "    test.to_csv(path + \"test_with_t5_questions.csv\")\n",
        "\n",
        "    bleu, rouge1, rouge2, rougeL, meteor = calculate_scores(test)\n",
        "\n",
        "    f = open(path + \"t5_questions_eval.txt\", \"a\")\n",
        "\n",
        "    f.write(\"Bleu: \" + str(bleu) + \"\\nRouge1: \" + str(rouge1) + \"\\nRouge2: \" + str(rouge2) + \"\\nRougeL: \" + str(rougeL) + \"\\nMeteor: \" + str(meteor))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    os.remove(\"checkpoints/bestcheckpoint.ckpt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxE5Q-zUTNXK"
      },
      "outputs": [],
      "source": [
        "!mkdir squad\n",
        "!mkdir hotpotqa\n",
        "!mkdir squad_folds\n",
        "!mkdir hotpotqa_folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Zp5utzTPaZ"
      },
      "outputs": [],
      "source": [
        "df_squad = get_squad()\n",
        "create_folds(df_squad, \"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH4ID9ARTROp"
      },
      "outputs": [],
      "source": [
        "df_hotpotqa = get_hotpotqa()\n",
        "create_folds(df_hotpotqa, \"hotpotqa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOOJMwKUT94T"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME ='t5-base' \n",
        "BATCH_SIZE = 8\n",
        "N_EPOCHS = 2\n",
        "DATASET = \"hotpotqa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP4hI_IKxoEI"
      },
      "outputs": [],
      "source": [
        "!unzip hotpotqa_folds_eval.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WObXNFD0V90x"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "  dirpath=\"checkpoints\",\n",
        "  filename=\"bestcheckpoint\",\n",
        "  save_top_k=1,\n",
        "  verbose=True,\n",
        "  monitor=\"val_loss\",\n",
        "  mode=\"min\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=checkpoint_callback,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    max_epochs = N_EPOCHS,\n",
        "    gpus=1,\n",
        "    progress_bar_refresh_rate=30\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3k9VflgILoZ"
      },
      "outputs": [],
      "source": [
        "cross_val(DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM8ryxAET25Z"
      },
      "outputs": [],
      "source": [
        "#!zip -r squad_folds_eval.zip squad_folds/\n",
        "!zip -r hotpotqa_folds_eval.zip hotpotqa_folds/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHbLBBYFTUyJ"
      },
      "outputs": [],
      "source": [
        "input_len_list = [len(x) for x in df_squad[\"input_text\"]]\n",
        "min_len = min(input_len_list)\n",
        "max_len = max(input_len_list)\n",
        "mean_len = sum(input_len_list) / len(input_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPiqB-zETowJ"
      },
      "outputs": [],
      "source": [
        "qa_input = [\"question: \" + df_squad.iloc[i,:][\"question\"] + \" context: \" + df_squad.iloc[i,:][\"context\"] + \" <\\s>\" for i in range(len(df_squad))]\n",
        "input_len_list = [len(x) for x in qa_input]\n",
        "min_len = min(input_len_list)\n",
        "max_len = max(input_len_list)\n",
        "mean_len = sum(input_len_list) / len(input_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t28tDotWTqLR"
      },
      "outputs": [],
      "source": [
        "output_len_list = [len(x) for x in df_squad[\"question\"]]\n",
        "min_len = min(output_len_list)\n",
        "max_len = max(output_len_list)\n",
        "mean_len = sum(output_len_list) / len(output_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMdUcy3cTr5Z"
      },
      "outputs": [],
      "source": [
        "output_len_list = [len(x) for x in df_squad[\"answer\"]]\n",
        "min_len = min(output_len_list)\n",
        "max_len = max(output_len_list)\n",
        "mean_len = sum(output_len_list) / len(output_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzMolwWeTvrB"
      },
      "outputs": [],
      "source": [
        "input_len_list = [len(x) for x in df_hotpotqa[\"input_text\"]]\n",
        "min_len = min(input_len_list)\n",
        "max_len = max(input_len_list)\n",
        "mean_len = sum(input_len_list) / len(input_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87WaHaFbTwSR"
      },
      "outputs": [],
      "source": [
        "qa_input = [\"question: \" + df_hotpotqa.iloc[i,:][\"question\"] + \" context1: \" + df_hotpotqa.iloc[i,:][\"context1\"] + \" context2: \" + df_hotpotqa.iloc[i,:][\"context2\"] + \" <\\s>\" for i in range(len(df_hotpotqa))]\n",
        "input_len_list = [len(x) for x in qa_input]\n",
        "min_len = min(input_len_list)\n",
        "max_len = max(input_len_list)\n",
        "mean_len = sum(input_len_list) / len(input_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxwXVgm3TxmR"
      },
      "outputs": [],
      "source": [
        "output_len_list = [len(x) for x in df_hotpotqa[\"answer\"]]\n",
        "min_len = min(output_len_list)\n",
        "max_len = max(output_len_list)\n",
        "mean_len = sum(output_len_list) / len(output_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJMk_tXyTy_y"
      },
      "outputs": [],
      "source": [
        "output_len_list = [len(x) for x in df_hotpotqa[\"question\"]]\n",
        "min_len = min(output_len_list)\n",
        "max_len = max(output_len_list)\n",
        "mean_len = sum(output_len_list) / len(output_len_list)\n",
        "print(\"Min: \" + str(min_len))\n",
        "print(\"Max: \" + str(max_len))\n",
        "print(\"Mean: \" + str(mean_len))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "T5 QG Evaluation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
